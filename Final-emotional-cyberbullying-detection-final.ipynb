{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8feb4929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\marit\\anaconda3\\anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm \n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from PIL import Image\n",
    "\n",
    "# Text analysis and NLP libraries\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Machine learning libraries\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, BatchNormalization, Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "import tensorflow as tf\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Download NLTK data at the beginning of the script\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a7d0e2",
   "metadata": {},
   "source": [
    "# Emotional Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503d35a",
   "metadata": {},
   "source": [
    "## Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4888d6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i beleive that i am much more sensitive to oth...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label\n",
       "0  i feel awful about it too because it s my job ...  sadness\n",
       "1                              im alone i feel awful  sadness\n",
       "2  ive probably mentioned this before but i reall...      joy\n",
       "3           i was feeling a little low few days back  sadness\n",
       "4  i beleive that i am much more sensitive to oth...     love"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the dataset\n",
    "kaggle_df = pd.read_csv('./data/tweet_emotions.csv').drop('tweet_id', axis=1)\n",
    "kaggle_df = kaggle_df.rename(columns={'content': 'text', 'sentiment': 'label'})\n",
    "huggingface_df = pd.read_parquet('./data/huggingface_emotions.parquet')\n",
    "# Replace numerical labels with corresponding emotions gotten from the metadata on hugging face's website\n",
    "label_to_emotion = {\n",
    "    0: 'sadness',\n",
    "    1: 'joy',\n",
    "    2: 'love',\n",
    "    3: 'anger',\n",
    "    4: 'fear',\n",
    "    5: 'surprise'\n",
    "}\n",
    "\n",
    "# Mapping the enocded labels to corresponding emotions \n",
    "huggingface_df['label'] = huggingface_df['label'].map(label_to_emotion)\n",
    "\n",
    "# Merge the datasets based on the common columns\n",
    "emotional_detection = pd.concat([huggingface_df, kaggle_df])\n",
    "\n",
    "# Display the first few rows of the merged dataset\n",
    "emotional_detection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e8921d",
   "metadata": {},
   "source": [
    "# Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87949e62",
   "metadata": {},
   "source": [
    "# Aggregate Duplicates by Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de2c172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31363</th>\n",
       "      <td>#frenchieb-day #frenchieb-day #frenchieb-day #...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29869</th>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39415</th>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30644</th>\n",
       "      <td>@JonathanRKnight BTW I STILL can't believe how...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39260</th>\n",
       "      <td>@RealHughJackman Wolverine is awesome.. love i...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37781</th>\n",
       "      <td>@andyclemmensen have you seen the game on the ...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10918</th>\n",
       "      <td>@ericbolling Where's Dani Babb?</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37807</th>\n",
       "      <td>@mari_possa Happy Happy Bday Baby Girl. Love Y...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30257</th>\n",
       "      <td>@mcraddictal</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32098</th>\n",
       "      <td>@thecompletes seen u a couple of times. Liked it</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33409</th>\n",
       "      <td>FREE UNLIMITED RINGTONES!!! - http://tinyurl.c...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33396</th>\n",
       "      <td>FREE UNLIMITED RINGTONES!!! - http://tinyurl.c...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33414</th>\n",
       "      <td>FREE UNLIMITED RINGTONES!!! - http://tinyurl.c...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33376</th>\n",
       "      <td>FREE UNLIMITED RINGTONES!!! - http://tinyurl.c...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33370</th>\n",
       "      <td>FREE UNLIMITED RINGTONES!!! - http://tinyurl.c...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33418</th>\n",
       "      <td>FREE UNLIMITED RINGTONES!!! - http://tinyurl.c...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27198</th>\n",
       "      <td>Good Morning</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28202</th>\n",
       "      <td>Good Morning</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32823</th>\n",
       "      <td>Good Morning</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text      label\n",
       "31363  #frenchieb-day #frenchieb-day #frenchieb-day #...    neutral\n",
       "29869                                                  0    neutral\n",
       "39415                                                  0    neutral\n",
       "30644  @JonathanRKnight BTW I STILL can't believe how...  happiness\n",
       "39260  @RealHughJackman Wolverine is awesome.. love i...       love\n",
       "37781  @andyclemmensen have you seen the game on the ...  happiness\n",
       "10918                    @ericbolling Where's Dani Babb?      worry\n",
       "37807  @mari_possa Happy Happy Bday Baby Girl. Love Y...       love\n",
       "30257                                       @mcraddictal    neutral\n",
       "32098   @thecompletes seen u a couple of times. Liked it       love\n",
       "33409  FREE UNLIMITED RINGTONES!!! - http://tinyurl.c...  happiness\n",
       "33396  FREE UNLIMITED RINGTONES!!! - http://tinyurl.c...    neutral\n",
       "33414  FREE UNLIMITED RINGTONES!!! - http://tinyurl.c...  happiness\n",
       "33376  FREE UNLIMITED RINGTONES!!! - http://tinyurl.c...  happiness\n",
       "33370  FREE UNLIMITED RINGTONES!!! - http://tinyurl.c...    neutral\n",
       "33418  FREE UNLIMITED RINGTONES!!! - http://tinyurl.c...    neutral\n",
       "27198                                       Good Morning    neutral\n",
       "28202                                       Good Morning       love\n",
       "32823                                       Good Morning    neutral"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the duplicated rows based on 'text' and 'label'\n",
    "duplicated_rows = emotional_detection[emotional_detection.duplicated(subset=['text', 'label'])]\n",
    "\n",
    "# Order the result by the 'text' column\n",
    "duplicated_rows_sorted = duplicated_rows.sort_values('text')\n",
    "duplicated_rows_sorted[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c90ae371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with aggregated labels for each text:\n",
      "                                                     text aggregated_label\n",
      "0       i feel awful about it too because it s my job ...          sadness\n",
      "1                                   im alone i feel awful          sadness\n",
      "2       ive probably mentioned this before but i reall...              joy\n",
      "3                i was feeling a little low few days back          sadness\n",
      "4       i beleive that i am much more sensitive to oth...             love\n",
      "...                                                   ...              ...\n",
      "501577                                   @JohnLloydTaylor          neutral\n",
      "501578                     Happy Mothers Day  All my love             love\n",
      "501579  Happy Mother's Day to all the mommies out ther...             love\n",
      "501580  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...        happiness\n",
      "501581  @mopedronin bullet train from tokyo    the gf ...             love\n",
      "\n",
      "[455989 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each label for each text\n",
    "label_counts = emotional_detection.groupby(['text', 'label']).size().reset_index(name='count')\n",
    "\n",
    "# Find the label with the highest count for each text\n",
    "idx = label_counts.groupby(['text'])['count'].transform(max) == label_counts['count']\n",
    "most_frequent_labels = label_counts[idx][['text', 'label']]\n",
    "\n",
    "# Merge the most frequent labels back to the original DataFrame\n",
    "aggregated_df = pd.merge(emotional_detection, most_frequent_labels, on='text', how='inner')\n",
    "\n",
    "# Drop duplicate rows based on the selected label\n",
    "aggregated_df = aggregated_df.drop_duplicates(subset=['text', 'label_y'])\n",
    "\n",
    "# Rename the 'label_y' column to 'aggregated_label'\n",
    "aggregated_df.rename(columns={'label_y': 'aggregated_label'}, inplace=True)\n",
    "\n",
    "# Print or display the result\n",
    "print(\"DataFrame with aggregated labels for each text:\")\n",
    "print(aggregated_df[['text', 'aggregated_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "298c72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_detection = aggregated_df[['text', 'aggregated_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89b8b267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with aggregated labels for each text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marit\\AppData\\Local\\Temp\\ipykernel_15120\\1610871232.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  emotional_detection.rename(columns={'aggregated_label': 'label'}, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i beleive that i am much more sensitive to oth...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501577</th>\n",
       "      <td>@JohnLloydTaylor</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501578</th>\n",
       "      <td>Happy Mothers Day  All my love</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501579</th>\n",
       "      <td>Happy Mother's Day to all the mommies out ther...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501580</th>\n",
       "      <td>@niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501581</th>\n",
       "      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455989 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text      label\n",
       "0       i feel awful about it too because it s my job ...    sadness\n",
       "1                                   im alone i feel awful    sadness\n",
       "2       ive probably mentioned this before but i reall...        joy\n",
       "3                i was feeling a little low few days back    sadness\n",
       "4       i beleive that i am much more sensitive to oth...       love\n",
       "...                                                   ...        ...\n",
       "501577                                   @JohnLloydTaylor    neutral\n",
       "501578                     Happy Mothers Day  All my love       love\n",
       "501579  Happy Mother's Day to all the mommies out ther...       love\n",
       "501580  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...  happiness\n",
       "501581  @mopedronin bullet train from tokyo    the gf ...       love\n",
       "\n",
       "[455989 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the 'aggregated_label' column to 'label'\n",
    "emotional_detection.rename(columns={'aggregated_label': 'label'}, inplace=True)\n",
    "# Print or display the result\n",
    "print(\"DataFrame with aggregated labels for each text:\")\n",
    "emotional_detection[['text', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815628d1",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19c5dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=emotional_detection['text']\n",
    "y=emotional_detection['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfabe55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the data into training and testing to avoid data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87e35590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train dataset\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "#test dataset\n",
    "test_df = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d67910c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'happiness' with 'joy' in the 'label' column since they are synonyms\n",
    "train_df['label'] = train_df['label'].replace('happiness', 'joy')\n",
    "test_df['label'] = test_df['label'].replace('happiness', 'joy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d89e9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_column(df, text_column_name):\n",
    "    # Make a copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Initialize the WordNet lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Define a regular expression pattern to remove punctuation\n",
    "    punctuation_pattern = r'[^\\w\\s]'\n",
    "\n",
    "    # Get the English stopwords list\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "    # Define a function for cleaning, tokenizing, and lemmatizing text\n",
    "    def preprocess_text(text):\n",
    "        # Remove punctuation and lowercase text\n",
    "        text = re.sub(punctuation_pattern, ' ', text.lower())\n",
    "\n",
    "        # Tokenize the text\n",
    "        words = text.split()\n",
    "\n",
    "        # Remove stop words and lemmatize each word\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in english_stopwords]\n",
    "\n",
    "        # Join the words back into a sentence\n",
    "        return ' '.join(words)\n",
    "\n",
    "    # Apply the preprocessing function to the text column of the copy\n",
    "    df_copy[text_column_name] = df_copy[text_column_name].apply(preprocess_text)\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddfc74a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed = preprocess_text_column(train_df, 'text')\n",
    "test_preprocessed = preprocess_text_column(test_df, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e329ac",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c806c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment Tagging\n",
    "def get_sentiment(text):\n",
    "    # Create a TextBlob object for the input text\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    # Get the polarity (-1 to 1) where -1 is negative, 1 is positive, and 0 is neutral\n",
    "    polarity = blob.sentiment.polarity\n",
    "    \n",
    "    # Determine the sentiment label based on polarity\n",
    "    if polarity > 0:\n",
    "        sentiment = 'positive'\n",
    "    elif polarity < 0:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "    \n",
    "    return sentiment, polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64f09664",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Polarity Score\n",
    "def get_text_polarity(text):\n",
    "    # Create a TextBlob object for the input text\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    # Get the polarity (-1 to 1) where -1 is negative, 1 is positive, and 0 is neutral\n",
    "    polarity = blob.sentiment.polarity\n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5bde9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the get_text_polarity function to the 'Text' column and create a new column 'Polarity'\n",
    "train_preprocessed['Polarity'] = train_preprocessed['text'].apply(get_text_polarity)\n",
    "test_preprocessed['Polarity'] = test_preprocessed['text'].apply(get_text_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c658052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the get_sentiment function to the 'Text' column and create new columns 'Sentiment Label' and 'Sentiment Polarity'\n",
    "train_preprocessed['Sentiment'], train_preprocessed['Polarity'] = zip(*train_preprocessed['text'].apply(get_sentiment))\n",
    "test_preprocessed['Sentiment'], test_preprocessed['Polarity'] = zip(*test_preprocessed['text'].apply(get_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cc7e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the preprocessed data\n",
    "X_train = train_preprocessed['text']\n",
    "y_train = train_preprocessed['label']\n",
    "X_test = test_preprocessed['text']\n",
    "y_test = test_preprocessed['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "470d1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "X_train = X_train.to_frame()\n",
    "# Identify categorical features based on data types\n",
    "categorical_features = [X_train[col].dtype == 'object' for col in X_train.columns]\n",
    "\n",
    "# Now add False for numerical features\n",
    "categorical_features += [False] * (len(X_train.columns) - sum(categorical_features))\n",
    "\n",
    "# Instantiate SMOTENC\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "\n",
    "# Instantiate SMOTEN\n",
    "sampling_strategy = {\n",
    "    'hate': 30000,\n",
    "    'enthusiasm': 30000,\n",
    "    'fun': 30000,\n",
    "    'empty': 30000,\n",
    "    'relief': 30000,\n",
    "    'boredom': 30000,\n",
    "    'worry': 20000,\n",
    "    'surprise': 15000,\n",
    "    'neutral': 15000,\n",
    "}\n",
    "\n",
    "smoten = SMOTEN(sampling_strategy=sampling_strategy, random_state=42)\n",
    "\n",
    "# Apply SMOTEN to generate synthetic samples\n",
    "X_train, y_train = smoten.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b8e0b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Flatten the NumPy array\n",
    "X_train_flat = X_train.values.flatten()\n",
    "\n",
    "# # Convert the flattened array to a Pandas Series\n",
    "X_train = pd.Series(X_train_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fed0b5",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53145b1",
   "metadata": {},
   "source": [
    "# BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f08cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\marit\\anaconda3\\anaconda\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\marit\\anaconda3\\anaconda\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\marit\\anaconda3\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\marit\\anaconda3\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "8139/8139 [==============================] - 473s 58ms/step - loss: 0.2629 - accuracy: 0.9022 - val_loss: 0.3180 - val_accuracy: 0.8779\n",
      "Epoch 2/20\n",
      "8139/8139 [==============================] - 528s 65ms/step - loss: 0.1879 - accuracy: 0.9281 - val_loss: 0.3250 - val_accuracy: 0.8811\n",
      "Epoch 3/20\n",
      "8139/8139 [==============================] - 476s 58ms/step - loss: 0.1732 - accuracy: 0.9315 - val_loss: 0.3136 - val_accuracy: 0.8811\n",
      "Epoch 4/20\n",
      "7566/8139 [==========================>...] - ETA: 32s - loss: 0.1599 - accuracy: 0.9345"
     ]
    }
   ],
   "source": [
    "# Define a mapping for your labels\n",
    "label_mapping = {'joy': 0, 'love': 1, 'fear': 2, 'anger': 3, 'neutral': 4, 'sadness': 5, 'surprise': 6, 'worry': 7, \n",
    "                 'hate':8, 'enthusiasm':9, 'fun':10, 'empty':11, 'relief':12, 'boredom':13}\n",
    "\n",
    "# Convert string labels to numerical labels using the mapping\n",
    "y_train_numeric = np.array([label_mapping[label] for label in y_train])\n",
    "y_test_numeric = np.array([label_mapping[label] for label in y_test])\n",
    "\n",
    "# Tokenize the text data\n",
    "max_words = 5000\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_sequence_length = max(len(seq) for seq in X_train_sequences)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Convert the target labels to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_numeric)\n",
    "y_test_onehot = to_categorical(y_test_numeric)\n",
    "\n",
    "# Build the BiLSTM model\n",
    "embedding_dim = 128\n",
    "num_classes = y_train_onehot.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(Bidirectional(LSTM(units=100)))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the BiLSTM model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Calculate class weights to address class imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_numeric), y=y_train_numeric)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the BiLSTM model\n",
    "history = model.fit(X_train_padded, y_train_onehot, validation_data=(X_test_padded, y_test_onehot),\n",
    "                    epochs=20, batch_size=64, class_weight=class_weight_dict, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the BiLSTM model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test_onehot)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predict on the test set with the BiLSTM model\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_classes = y_pred.argmax(axis=-1)\n",
    "\n",
    "# Convert numerical labels back to string labels using the reverse mapping\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "y_test_original = np.array([reverse_label_mapping[label] for label in y_test_numeric])\n",
    "y_pred_original = np.array([reverse_label_mapping[label] for label in y_pred_classes])\n",
    "\n",
    "\n",
    "# Evaluate the BiLSTM model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test_onehot)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_original, y_pred_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac13a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the BiLSTM model\n",
    "model.save('./models/BILSTM for Transfer Learning/bilstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e231d",
   "metadata": {},
   "source": [
    "# Transfer Learning to Cyberbullying Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d90d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data are sources from twitter, kaggle, wikepedia talk page, youtube\n",
    "aggression = pd.read_csv('data/aggression_parsed_dataset.csv')\n",
    "attack = pd.read_csv('data/attack_parsed_dataset.csv')\n",
    "toxicity = pd.read_csv('data/toxicity_parsed_dataset.csv')\n",
    "racism = pd.read_csv('data/twitter_racism_parsed_dataset.csv')\n",
    "sexism = pd.read_csv('data/twitter_sexism_parsed_dataset.csv')\n",
    "kaggle = pd.read_csv('data/kaggle_parsed_dataset.csv')\n",
    "twitter = pd.read_csv('data/twitter_parsed_dataset.csv')\n",
    "youtube = pd.read_csv('data/youtube_parsed_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to each DataFrame indicating the source dataset\n",
    "aggression['source'] = 'aggression'\n",
    "attack['source'] = 'attack'\n",
    "toxicity['source'] = 'toxicity'\n",
    "racism['source'] = 'racism'\n",
    "sexism['source'] = 'sexism'\n",
    "kaggle['source'] = 'kaggle'\n",
    "twitter['source'] = 'twitter'\n",
    "youtube['source'] = 'youtube'\n",
    "\n",
    "# Concatenate all DataFrames along with the newly added 'source' column\n",
    "cyberbullying_data = pd.concat([aggression, attack, toxicity, racism, sexism, kaggle, twitter, youtube], ignore_index=True)\n",
    "\n",
    "# Now you have a single DataFrame containing all the data from different sources\n",
    "# with an additional 'source' column to indicate the dataset origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4323a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of column names to drop\n",
    "columns_to_drop = ['id', 'Annotation', 'Date', 'UserIndex',\n",
    "                   'Number of Comments', 'Number of Subscribers', 'Membership Duration',\n",
    "                   'Number of Uploads', 'Profanity in UserID', 'Age', 'index']\n",
    "\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "cyberbullying_data = cyberbullying_data.drop(columns=columns_to_drop)\n",
    "\n",
    "cyberbullying_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5023458",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bd0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates based on the 'Text' column\n",
    "cyberbullying_data = cyberbullying_data.drop_duplicates(subset=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns ed_label_0 and ed_label_1\n",
    "cyberbullying_data = cyberbullying_data.drop(['ed_label_0', 'ed_label_1'], axis=1)\n",
    "\n",
    "# Rename column oh_label to cyberbullying\n",
    "cyberbullying_data = cyberbullying_data.rename(columns={'oh_label': 'cyberbullying'})\n",
    "\n",
    "# Display the modified DataFrame\n",
    "cyberbullying_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyberbullying_data = cyberbullying_data.dropna(subset=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214cbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved tokenizer\n",
    "new_data_sequences = tokenizer.texts_to_sequences(cyberbullying_data['Text'])\n",
    "new_data_padded = pad_sequences(new_data_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "new_data_predictions = model.predict(new_data_padded)\n",
    "new_data_predictions_classes = new_data_predictions.argmax(axis=-1)\n",
    "\n",
    "emotion_cyberbullying_data = np.array([reverse_label_mapping[label] for label in new_data_predictions_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c963cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyberbullying_data.loc[:, 'Emotion_Label'] = emotion_cyberbullying_data\n",
    "cyberbullying_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0aa909",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyberbullying_data_processed = preprocess_text_column(cyberbullying_data, 'Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55516a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyberbullying_data_processed.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee56ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyberbullying_data_processed.to_csv('./data/cyberbullying_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051bd2de",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cyberbullying_data_processed['Text'],\n",
    "    cyberbullying_data_processed[['cyberbullying', 'Emotion_Label']],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdc1e16",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2387c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already imported and processed your data (X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Convert labels to strings\n",
    "y_train['cyberbullying'] = y_train['cyberbullying'].astype(str)\n",
    "y_train['Emotion_Label'] = y_train['Emotion_Label'].astype(str)\n",
    "\n",
    "y_test['cyberbullying'] = y_test['cyberbullying'].astype(str)\n",
    "y_test['Emotion_Label'] = y_test['Emotion_Label'].astype(str)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_words = 10000  # You can adjust this based on your data\n",
    "max_len = 100  # You can adjust this based on your data\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Convert labels to binary format\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_binary = mlb.fit_transform(y_train.values)\n",
    "y_test_binary = mlb.transform(y_test.values)\n",
    "\n",
    "# Build the neural network model (LSTM + CNN)\n",
    "num_classes = len(mlb.classes_)\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
    "    Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(X_train_padded, y_train_binary, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "y_pred_labels = mlb.inverse_transform(y_pred_binary)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_binary = mlb.transform(y_test.values)\n",
    "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a00870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model to a HDF5 file\n",
    "model.save(\"./models/emotioncyberbullying/NN_cyberbullying_emotion.h5\")\n",
    "\n",
    "# Save MultiLabelBinarizer\n",
    "with open(\"./models/emotioncyberbullying/mlb.pkl\", \"wb\") as mlb_file:\n",
    "    pickle.dump(mlb, mlb_file)\n",
    "\n",
    "# Save Tokenizer\n",
    "with open(\"./models/emotioncyberbullying/tokenizer.pkl\", \"wb\") as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentence\n",
    "new_sentence = \"This is an example sentence for prediction.\"\n",
    "\n",
    "# Preprocess and tokenize the new sentence\n",
    "new_sentence_seq = tokenizer.texts_to_sequences([new_sentence])\n",
    "new_sentence_padded = pad_sequences(new_sentence_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Predict using the trained model\n",
    "new_sentence_pred = model.predict(new_sentence_padded)\n",
    "new_sentence_pred_binary = (new_sentence_pred > 0.5).astype(int)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "new_sentence_labels = mlb.inverse_transform(new_sentence_pred_binary)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(\"Predicted Labels:\", new_sentence_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d71f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentence\n",
    "new_sentence = \"I hate you\"\n",
    "\n",
    "# Preprocess and tokenize the new sentence\n",
    "new_sentence_seq = tokenizer.texts_to_sequences([new_sentence])\n",
    "new_sentence_padded = pad_sequences(new_sentence_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Predict using the trained model\n",
    "new_sentence_pred = model.predict(new_sentence_padded)\n",
    "new_sentence_pred_binary = (new_sentence_pred > 0.5).astype(int)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "new_sentence_labels = mlb.inverse_transform(new_sentence_pred_binary)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(\"Predicted Labels:\", new_sentence_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf74e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentence\n",
    "new_sentence = \"what you are doing is bad\"\n",
    "\n",
    "# Preprocess and tokenize the new sentence\n",
    "new_sentence_seq = tokenizer.texts_to_sequences([new_sentence])\n",
    "new_sentence_padded = pad_sequences(new_sentence_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Predict using the trained model\n",
    "new_sentence_pred = model.predict(new_sentence_padded)\n",
    "new_sentence_pred_binary = (new_sentence_pred > 0.5).astype(int)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "new_sentence_labels = mlb.inverse_transform(new_sentence_pred_binary)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(\"Predicted Labels:\", new_sentence_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef80bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentence\n",
    "new_sentence = \"I hate when I think about you\"\n",
    "\n",
    "# Preprocess and tokenize the new sentence\n",
    "new_sentence_seq = tokenizer.texts_to_sequences([new_sentence])\n",
    "new_sentence_padded = pad_sequences(new_sentence_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Predict using the trained model\n",
    "new_sentence_pred = model.predict(new_sentence_padded)\n",
    "new_sentence_pred_binary = (new_sentence_pred > 0.5).astype(int)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "new_sentence_labels = mlb.inverse_transform(new_sentence_pred_binary)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(\"Predicted Labels:\", new_sentence_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentence\n",
    "new_sentence = \"I hate when I think about the future\"\n",
    "\n",
    "# Preprocess and tokenize the new sentence\n",
    "new_sentence_seq = tokenizer.texts_to_sequences([new_sentence])\n",
    "new_sentence_padded = pad_sequences(new_sentence_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Predict using the trained model\n",
    "new_sentence_pred = model.predict(new_sentence_padded)\n",
    "new_sentence_pred_binary = (new_sentence_pred > 0.5).astype(int)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "new_sentence_labels = mlb.inverse_transform(new_sentence_pred_binary)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(\"Predicted Labels:\", new_sentence_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f8a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentence\n",
    "new_sentence = \"I am so happy today\"\n",
    "\n",
    "# Preprocess and tokenize the new sentence\n",
    "new_sentence_seq = tokenizer.texts_to_sequences([new_sentence])\n",
    "new_sentence_padded = pad_sequences(new_sentence_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Predict using the trained model\n",
    "new_sentence_pred = model.predict(new_sentence_padded)\n",
    "new_sentence_pred_binary = (new_sentence_pred > 0.5).astype(int)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "new_sentence_labels = mlb.inverse_transform(new_sentence_pred_binary)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(\"Predicted Labels:\", new_sentence_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2891c0",
   "metadata": {},
   "source": [
    "# Model Comparaison by Predicting on New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4142beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(\"data/Twitter_CyberBullying_Comments_Unseen_Dataset.csv\")\n",
    "new_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7779ca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "loaded_model = load_model('./models/emotioncyberbullying/NN_cyberbullying_emotion.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15249109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing the new data\n",
    "X_new_seq = tokenizer.texts_to_sequences(new_data['Text'])\n",
    "X_new_padded = pad_sequences(X_new_seq, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4b44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the new data\n",
    "y_new_pred = loaded_model.predict(X_new_padded)\n",
    "y_new_pred_binary = (y_new_pred > 0.5).astype(int)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "y_new_pred_labels = mlb.inverse_transform(y_new_pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6610615",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['predicted_labels'] = y_new_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64894ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['predicted_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c3170",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['CB_pred']=new_data['predicted_labels'].str[0]\n",
    "new_data['emotion_pred']=new_data['predicted_labels'].str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f22fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming new_data is your DataFrame\n",
    "new_data['CB_pred'] = pd.to_numeric(new_data['CB_pred'], errors='coerce')\n",
    "new_data['CB_pred'] = new_data['CB_pred'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070605b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 2 predictions\n",
    "fairnesscyberbullying_pred=pd.read_csv('data/fairnesscyberbullying_pred.csv')\n",
    "fairnesscyberbullying_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e995c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ieee_model_accuracy = accuracy_score(fairnesscyberbullying_pred['CB_Label'], fairnesscyberbullying_pred['fairnesscyberbullying_pred'])\n",
    "ieee_model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb33c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel_accuracy = accuracy_score(new_data['CB_Label'], new_data['CB_pred'])\n",
    "mymodel_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be342b8f",
   "metadata": {},
   "source": [
    "# Paired T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the accuracy scores\n",
    "print(\"IEEE Model:\", ieee_model_accuracy)\n",
    "print(\"My model:\", mymodel_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b379fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Sample data for Model 1 and Model 2 (replace with your actual data)\n",
    "model1_predictions = fairnesscyberbullying_pred['fairnesscyberbullying_pred']\n",
    "model2_predictions = new_data['CB_pred']\n",
    "true_labels = fairnesscyberbullying_pred['CB_Label']  # Assuming CB_Label is the true label column\n",
    "\n",
    "# Calculate the differences in accuracy between Model 1 and Model 2\n",
    "differences = ieee_model_accuracy - mymodel_accuracy\n",
    "\n",
    "# Perform the Paired T-Test\n",
    "t_statistic, p_value = stats.ttest_rel(model1_predictions, model2_predictions)\n",
    "\n",
    "# Set your chosen significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Print the Paired T-Test results\n",
    "print(\"Paired T-Test Results:\")\n",
    "print(f\"t-statistic: {t_statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Check if the p-value is less than the significance level\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference in performance.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference in performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bcaa03",
   "metadata": {},
   "source": [
    "# McNemar's Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867714a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "new_model1_predictions = model1_predictions\n",
    "new_model2_predictions = model2_predictions\n",
    "new_true_labels = new_data['CB_Label']\n",
    "\n",
    "# Convert predictions to binary (assuming they are probabilities)\n",
    "threshold = 0.5\n",
    "new_model1_binary = (new_model1_predictions > threshold).astype(int)\n",
    "new_model2_binary = (new_model2_predictions > threshold).astype(int)\n",
    "\n",
    "# Create a confusion matrix for the new data\n",
    "new_conf_matrix = confusion_matrix(new_true_labels, new_model2_binary)  # Fix: Use new_model2_binary here\n",
    "\n",
    "# Extract values from the new confusion matrix\n",
    "new_a = new_conf_matrix[0, 0]\n",
    "new_b = new_conf_matrix[0, 1]\n",
    "new_c = new_conf_matrix[1, 0]\n",
    "new_d = new_conf_matrix[1, 1]\n",
    "\n",
    "# Perform McNemar's test for the new data\n",
    "new_statistic = ((new_b - new_c) ** 2) / (new_b +  new_c)\n",
    "new_p_value = chi2_contingency([[new_b, new_c], [new_d, new_a]])[1]\n",
    "\n",
    "# Set your chosen significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Print McNemar's test results for the new data\n",
    "print(\"New McNemar's Test Results:\")\n",
    "print(f\"Chi-squared statistic: {new_statistic}\")\n",
    "print(f\"p-value: {new_p_value}\")\n",
    "\n",
    "# Check if the p-value is less than the significance level\n",
    "if new_p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference in predictions.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference in predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352fb414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
